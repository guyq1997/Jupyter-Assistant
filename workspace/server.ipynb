{"cells":[{"id":"12104743-5770-471d-a050-a065eca18ae1","cell_type":"markdown","source":[" Part 1 - Deploying a Machine Learning ModelWelcome to this ungraded lab!This lab is all about deploying a real machine learning model, and checking what doing so feels like. More concretely, you will deploy a computer vision model trained to detect common objects in pictures. Deploying a model is one of the last steps in a prototypical machine learning lifecycle. However, we thought it would be exciting to get you to deploy a model right away. This lab uses a pretrained model called [`YOLOV3`](https://pjreddie.com/darknet/yolo/). This model is very convenient for two reasons: it runs really fast, and for object detection it yields accurate results.The sequence of steps/tasks to complete in this lab are as follow:1. Inspect the image data set used for object detection.2. Take a look at the model itself.3. Deploy the model using fastAPI. You can check its website [here](https://fastapi.tiangolo.com/).Here is a [shortcut](#instructions) to the instructions on how to interact with your model once it has been deployed. **This will be useful later on, for now just continue with the notebook as usual.**"]},{"id":"b310cb76-f4d4-4e11-9be4-599639ff8f24","cell_type":"markdown","source":["## Object Detection with YOLOV3\n","\n","### Inspecting the images\n","\n","Let's take a look at the images that will be passed to the YOLOV3 model. This will bring insight on what type of common objects are present for detection. These images are part of the [`ImageNet`](http://www.image-net.org/index) dataset and are stored within the `images` directory within this environment."]},{"id":"60e53be9-786f-4981-98dd-db42875a4c91","cell_type":"code","source":["from IPython.display import Image, d","isplay"]},{"id":"3a6f9f17-22b3-44d1-be9f-f78db899ee96","cell_type":"code","source":["# Some example imagesimage_files = [    'apple.jpg',    'clock.jpg',    'oranges.jpg',    'car.jpg']for image_file in image_files:    print(f\"\\nDisplaying image: {image_file}\")    display(Image(filename=f\"images/{image_file}\"))",""]},{"id":"787fb7e5-78de-472b-8f9c-d1d34d1ec27c","cell_type":"markdown","source":["### Overview of the model\n","\n","Now that you have a sense of the image data and the objects present, let's try and see if the model is able to detect and classify them correctly.\n","\n","For this you will be using [`cvlib`](https://www.cvlib.net/), which is a very simple but powerful library for object detection that is fueled by [`OpenCV`](https://docs.opencv.org/4.5.1/) and [`Tensorflow`](https://www.tensorflow.org/).\n","\n","More concretely, you will use the [`detect_common_objects`](https://docs.cvlib.net/object_detection/) function, which takes an image formatted as a [`numpy array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) and returns:\n","\n","- `bbox`: list of list containing bounding box coordinates for detected objects. \n","\n","        Example:\n","    \n","    ```python\n","        [[32, 76, 128, 192], [130, 83, 220, 185]]\n","    ```\n","    \n","\n","- `label`: list of labels for detected objects.\n","    \n","        Example:\n","    ```python\n","        ['apple', 'apple']\n","    ```\n","\n","\n","- `conf`: list of confidence scores for detected objects.\n","        Example:\n","        \n","    ```python\n","        [0.6187325716018677, 0.42835739254951477]\n","    ```\n","    \n","In the next section you will visually see these elements in action."]},{"id":"19e9666f-0a8e-4ff3-9231-3b3b86244fdf","cell_type":"markdown","source":["### Creating the detect_and_draw_box function"]},{"id":"b0c02623-22ab-41d9-913b-9154c353f128","cell_type":"markdown","source":["Let's define the `detect_and_draw_box` function which takes as input arguments: \n","\n","- the **filename** of a file on your system\n","- a **model**\n","- a **confidence level**. \n","\n","With these inputs, it detects common objects in the image and saves a new image displaying the bounding boxes alongside the detected object. These new images will be saved within the `images_with_boxes` directory.\n","\n","You might ask yourself why does this function receive the model as an input argument? What models are there to choose from? The answer is that `detect_common_objects` uses the `yolov3` model by default.  However, there is another option available that is much tinier and requires less computational power. \n","\n","It is the `yolov3-tiny` version. As the model name indicates, this model is designed for constrained environments that cannot store big models. With this comes a natural tradeoff: the results are less accurate than the full model. However, it still works pretty well. Going forward you can use whichever you prefer but by default `yolov3-tiny` will be used.\n","\n","The model output is a vector of probabilities for the presence of different objects on the image. The last input argument, confidence level, determines the threshold that the probability needs to surpass to report that a given object is detected on the supplied image. By default, `detect_common_objects` uses a value of 0.5 for this."]},{"id":"5a50dec7-5014-487c-96a4-85daecd73d14","cell_type":"code","source":["import cv2\n","import cvlib as cv\n","from cvlib.object_detection import draw_bbox\n","\n","\n","def detect_and_draw_box(filename, model=\"yolov3-tiny\", confidence=0.5):\n","    \"\"\"Detects common objects on an image and creates a new image with bounding boxes.\n","\n","    Args:\n","        filename (str): Filename of the image.\n","        model (str): Either \"yolov3\" or \"yolov3-tiny\". Defaults to \"yolov3-tiny\".\n","        confidence (float, optional): Desired confidence level. Defaults to 0.5.\n","    \"\"\"\n","    \n","    # Images are stored under the images/ directory\n","    img_filepath = f'images/{filename}'\n","    \n","    # Read the image into a numpy array\n","    img = cv2.imread(img_filepath)\n","    \n","    # Perform the object detection\n","    bbox, label, conf = cv.detect_common_objects(img, confidence=confidence, model=model)\n","    \n","    # Print current image's filename\n","    print(f\"========================\\nImage processed: {filename}\\n\")\n","    \n","    # Print detected objects with confidence level\n","    for l, c in zip(label, conf):\n","        print(f\"Detected object: {l} with confidence level of {c}\\n\")\n","    \n","    # Create a new image that includes the bounding boxes\n","    output_image = draw_bbox(img, bbox, label, conf)\n","    \n","    # Save the image in the directory images_with_boxes\n","    cv2.imwrite(f'images_with_boxes/{filename}', output_image)\n","    \n","    # Display the image with bounding boxes\n","    display(Image(f'images_with_boxes/{filename}'))"]},{"id":"cec58a48-ef55-43b5-8ff1-1b71995461f5","cell_type":"markdown","source":["Let's try it out for the example images."]},{"id":"19de0ebd-e6dd-43b5-ab7f-0dbff8bac858","cell_type":"code","source":["for image_file in image_files:\n","    detect_and_draw_box(image_file)"]},{"id":"bf37b025-24a2-4216-92bf-3e47e5c2a2ad","cell_type":"markdown","source":["## Changing the confidence level\n","\n","Looks like the object detection went fairly well. Let's try it out on a more difficult image containing several objects:"]},{"id":"f293e157-5456-4e87-b45d-99b0d64a7174","cell_type":"code","source":["detect_and_draw_box(\"fruits.jpg\")"]},{"id":"21b17151-36a9-4174-803f-2c131cd8444a","cell_type":"markdown","source":["The **model failed to detect** several fruits and **misclassified** an orange as an apple. This might seem strange since it was able to detect one apple before, so one might think the model has a fair representation on how an apple looks like.\n","\n","One possibility is that the model **did** detect the other fruits but with a confidence level lower than 0.5. Let's test if  this is a valid hypothesis:"]},{"id":"352f5361-5136-428e-907a-2cc32e6dd6b6","cell_type":"code","source":["detect_and_draw_box(\"fruits.jpg\", confidence=0.2)",""]},{"id":"0412598e-0dad-4b41-b58d-35374c18eff6","cell_type":"markdown","source":["By lowering the confidence level the model successfully detects most of the fruits. However, in order to correctly detect the objects present, we had to  set the confidence level really low. In general, you should be careful when decreasing or increasing these kinds of parameters, as changing them might yield undesired results.\n","\n","As for this concrete example when an orange was misclassified as an apple, **it serves as a reminder that these models are not perfect and this should be considered when using them for tasks in production**."]},{"id":"6af5763e-676d-4668-b771-cd26ebade213","cell_type":"markdown","source":["## Deploying the model using fastAPI\n","\n","\n","### Placing your object detection model in a server\n","\n","Now that you know how the model works it is time for you to deploy it! Aren't you excited? :)\n","\n","Before diving into deployment, let's quickly recap some important concepts and how they translate to `fastAPI`. The images that are uploaded to the server will be used within the `images_uploaded` directory.\n"]},{"id":"dafca0b7-21e5-4a4a-b2b3-f8ed5987d7c1","cell_type":"markdown","source":["### Some concept clarifications\n","\n","#### Client-Server model\n","\n","When talking about **deploying**, what is usually meant is to put all of the software required for predicting in a `server`. By doing this, a `client` can interact with the model by sending `requests` to the server. \n","\n","This client-server interaction is out of the scope of this notebook but there are a lot of resources on the internet that you can use to understand it better.\n","\n","The important thing you need to focus on, is that the Machine Learning model lives in a server waiting for clients to submit prediction requests. The client should provide the required information that the model needs in order to make a prediction. Keep in mind that it is common to batch many predictions in a single request. The server will use the information provided to return predictions to the client, who can then use them at their leisure.\n","\n","Let's get started by creating an instance of the `FastAPI` class:\n","\n","```python\n","app = FastAPI()\n","```\n","\n","The next step is using this instance to create endpoints that will handle the logic for predicting (more on this next). Once all the code is in place to run the server you only need to use the command:\n","\n","```python\n","uvicorn.run(app)\n","```\n","\n","Your API is coded using fastAPI but the serving is done using [`uvicorn`](https://www.uvicorn.org/), which is a really fast Asynchronous Server Gateway Interface (ASGI) implementation. Both technologies are closely interconnected and you don't need to understand the implementation details. Knowing that uvicorn handles the serving is sufficient for the purpose of this lab.\n","\n","#### Endpoints\n","\n","You can host multiple Machine Learning models on the same server. For this to work, you can assign a different `endpoint` to each model so you always know what model is being used. An endpoint is represented by a pattern in the `URL`. For example, if you have a website called `myawesomemodel.com` you could have three different models in the following endpoints:\n","\n","- `myawesomemodel.com/count-cars/`\n","- `myawesomemodel.com/count-apples/`\n","- `myawesomemodel.com/count-plants/`\n","\n","Each model would do what the name pattern suggests.\n","\n","In fastAPI you define an endpoint by creating a function that will handle all of the logic for that endpoint and [decorating](https://www.python.org/dev/peps/pep-0318/) it with a function that contains information on the HTTP method allowed (more on this next) and the pattern in the URL that it will use.\n","\n","The following example shows how to allow a HTTP GET request for the endpoint \"/my-endpoint\":\n","\n","```python\n","@app.get(\"/my-endpoint\")\n","def handle_endpoint():\n","    ...\n","    ...\n","```\n","\n","\n","#### HTTP Requests\n","\n","The client and the server communicate with each other through a protocol called `HTTP`. The key concept here is that this communication between client and server uses some verbs to denote common actions. Two very common verbs are:\n","\n","- `GET` -> Retrieves information from the server.\n","- `POST` -> Provides information to the server, which it uses to respond.\n","\n","If your client does a `GET request` to an endpoint of a server you will get some information from this endpoint without the need to provide additional information. In the case of a `POST request` you are explicitly telling the server that you will provide some information for it that must be processed in some way.\n","\n","Interactions with Machine Learning models living on endpoints are usually done via a `POST request` since you need to provide the information that is required to compute a prediction.\n","\n","Let's take a look at a POST request:\n","\n","```python\n","@app.post(\"/my-other-endpoint\")\n","def handle_other_endpoint(param1: int, param2: str):\n","    ...\n","    ...\n","\n","```\n","\n","For POST requests, the handler function contains parameters. In contrast with GET, POST requests expect the client to provide some information to it. In this case we supplied two parameters: an integer and a string.\n","\n","\n","### Why fastAPI?\n","\n","With fastAPI you can create web servers to host your models very easily. Additionally, this platform is extremely fast and it **has a built-in client that can be used to interact with the server**. To use it you will need to visit the \"/docs\" endpoint, you will see how to do this later on. Isn't that convenient?\n","\n","Enough chatter, let's get going!"]},{"id":"12b2d9a2-d893-408a-af61-8daf1fa003b6","cell_type":"code","source":["import io\n","import uvicorn\n","import numpy as np\n","import nest_asyncio\n","from enum import Enum\n","from fastapi import FastAPI, UploadFile, File, HTTPException\n","from fastapi.responses import StreamingResponse"]},{"id":"214200d6-5d29-4c32-afe9-2c70d2753934","cell_type":"code","source":["# Assign an instance of the FastAPI class to the variable \"app\".\n","# You will interact with your api using this instance.\n","app = FastAPI(title='Deploying a ML Model with FastAPI')\n","\n","# List available models using Enum for convenience. This is useful when the options are pre-defined.\n","class Model(str, Enum):\n","    yolov3tiny = \"yolov3-tiny\"\n","    yolov3 = \"yolov3\"\n","\n","\n","# By using @app.get(\"/\") you are allowing the GET method to work for the / endpoint.\n","@app.get(\"/\")\n","def home():\n","    return \"Congratulations! Your API is working as expected.\"\n","\n","\n","# This endpoint handles all the logic necessary for the object detection to work.\n","# It requires the desired model and the image in which to perform object detection.\n","@app.post(\"/predict\") \n","def prediction(model: Model, file: UploadFile = File(...)):\n","\n","    # 1. VALIDATE INPUT FILE\n","    filename = file.filename\n","    fileExtension = filename.split(\".\")[-1] in (\"jpg\", \"jpeg\", \"png\")\n","    if not fileExtension:\n","        raise HTTPException(status_code=415, detail=\"Unsupported file provided.\")\n","    \n","    # 2. TRANSFORM RAW IMAGE INTO CV2 image\n","    \n","    # Read image as a stream of bytes\n","    image_stream = io.BytesIO(file.file.read())\n","    \n","    # Start the stream from the beginning (position zero)\n","    image_stream.seek(0)\n","    \n","    # Write the stream of bytes into a numpy array\n","    file_bytes = np.asarray(bytearray(image_stream.read()), dtype=np.uint8)\n","    \n","    # Decode the numpy array as an image\n","    image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n","    \n","    \n","    # 3. RUN OBJECT DETECTION MODEL\n","    \n","    # Run object detection\n","    bbox, label, conf = cv.detect_common_objects(image, model=model)\n","    \n","    # Create image that includes bounding boxes and labels\n","    output_image = draw_bbox(image, bbox, label, conf)\n","    \n","    # Save it in a folder within the server\n","    cv2.imwrite(f'images_uploaded/{filename}', output_image)\n","    \n","    \n","    # 4. STREAM THE RESPONSE BACK TO THE CLIENT\n","    \n","    # Open the saved image for reading in binary mode\n","    file_image = open(f'images_uploaded/{filename}', mode=\"rb\")\n","    \n","    # Return the image as a stream specifying media type\n","    return StreamingResponse(file_image, media_type=\"image/jpeg\")"]},{"id":"90773578-e8b3-4ede-8209-c8c83f9b390d","cell_type":"markdown","source":["By running the following cell you will spin up the server!\n","\n","This causes the notebook to block (no cells/code can run) until you manually interrupt the kernel. You can do this by clicking on the `Kernel` tab and then on `Interrupt`. You can also enter Jupyter's command mode by pressing the `ESC` key and tapping the `I` key twice."]},{"id":"dfb3493b-47ff-4ce8-9e37-d9b577ca7199","cell_type":"code","source":["# Allows the server to be run in this interactive environment\n","nest_asyncio.apply()\n","\n","# This is an alias for localhost which means this particular machine\n","host = \"127.0.0.1\"\n","\n","# Spin up the server!    \n","uvicorn.run(app, host=host, port=8000, root_path=\"/serve\")"]},{"id":"46e4fa86-9456-44a1-8d45-64aa123aed37","cell_type":"markdown","source":["The server is now running! Nice job!"]},{"id":"c880c2fd-0485-4607-ace1-2d4af0888ab1","cell_type":"markdown","source":["## Consume your service\n","\n","Normally you will now head over to `http://127.0.0.1:8000/` to see it in action. However Coursera environment works somewhat different to a regular pc. Within this environment you need to interact with the service through the navigation bar, which can be found in the upper side of your screen:"]},{"id":"67aeba85-d4ef-4c77-8244-b7a2ef8d3d77","cell_type":"markdown","source":["<table><tr><td><img src='assets/notebook_path.png'></td></tr></table>"]},{"id":"3d28f216-09b8-47a9-865c-edac1375400c","cell_type":"markdown","source":["If you don't see this bar you might need to click on the `Navigate` button first:"]},{"id":"baa0798b-aead-4d1a-a690-58fabe6063a9","cell_type":"markdown","source":["\n","<table><tr><td><img src='assets/navigate.png'></td></tr></table>"]},{"id":"b3727b85-8e1a-4ca6-867a-29cdf7c75776","cell_type":"markdown","source":["### Come back to this notebook\n","\n","To come back you have two alternatives:\n","\n","- Click the `Home` button at the left side of the navigation bar. \n","\n","\n","- Type `/notebooks/server.ipynb` in the navigation bar and press enter.\n","\n","### Visit the server\n","\n","You can think of `/serve/` as an alias for `http://127.0.0.1:8000/`.\n","\n","With this in mind, to interact with the server you need to type `/serve/` in this bar and press enter.\n","\n","This will take you to the `/` endpoint of the server, which should display the message `Congratulations! Your API is working as expected`.\n"]},{"id":"282f7b2e-c1a8-46cd-833d-50a16874da86","cell_type":"markdown","source":["## Using fastAPI's integrated client"]},{"id":"1c7c2e17-35b3-43e5-a6e3-7822f4ba00ec","cell_type":"markdown","source":["To actually use your server for image detection you can leverage the client that comes built-in with fastAPI. \n","\n","**To use this client type `/serve/docs` in the navigation bar and press enter.**\n","\n","<table><tr><td><img src='assets/serve_docs.png'></td></tr></table>\n","\n","Try submitting an image and see how your API is able to detect objects within it and return a new image containing the bounding boxes alongside the labels of the detected objects.\n","\n","When doing so you will get a screen that should look like the one below, follow the instructions next:\n","\n","<a name='instructions'></a>\n","## Instructions to use the client\n","\n","**Note:** If you need to come back to this notebook to check these instructions you can do so as explained earlier. Remember that at the top of the notebook there is a shortcut to this section so you don't have to scroll all the way.\n","\n","Click on top of the `/predict` endpoint and more options will become visible:"]},{"id":"f1ca945c-fba0-4648-9889-05319fbe019c","cell_type":"markdown","source":["![image.png](attachment:image.png)\n","\n"]},{"id":"17fa1121-88e2-40ba-81eb-8d11b7f0ad6f","cell_type":"markdown","source":["To test your server click on the **Try it out** button."]},{"id":"33c424c5-542c-482a-9789-49f88bf60d70","cell_type":"markdown","source":["![image.png](attachment:image.png)"]},{"id":"a5dd204f-4db3-4d5e-aed3-ecc283bba826","cell_type":"markdown","source":["You can choose a model from the **model** field and a **file** which should be the image in which you want the server to detect objects.\n","\n","**Submit an image** from your local filesystem by clicking the **Choose File** button, then click on the blue **Execute** button to send an HTTP request to the server. After doing so, **scroll down and you will see the response from it**. Pretty cool, right?"]},{"id":"e5f30660-c272-488b-9cdf-57e62fe3aa4b","cell_type":"markdown","source":["![image.png](attachment:image.png)"]},{"id":"ecb0157f-c058-4ce3-9187-33b6fde017e6","cell_type":"markdown","source":["**Try different images!** You can use the ones we provided with this lab or some of your own. Since the model is using the default confidence level of 0.5 it might not always succeed to detect some objects. \n","\n","To download the images provided with lab follow these steps:\n","\n","- Click on the Jupyter logo in the upper left side of the screen. This will take you to the Jupyter filesystem.\n","\n","\n","- Go into the `images` directory.\n","\n","\n","- Here you can select any image and then click on the download button to download it.\n","\n","Also, try submitting non-image files and see how the server reacts to it."]},{"id":"9fa8e31a-4c4c-4d36-b3b6-924cdf8a6e70","cell_type":"markdown","source":["## Congratulations on finishing this ungraded lab!\n","\n","Real life servers have a lot more going on in terms of security and performance. However, the code you just experienced is close to what you see in real production environments. Hopefully, this lab served the purpose of increasing your familiarity with the process of deploying a Deep Learning model, and consuming from it.\n","\n","**Keep it up!**"]},{"id":"7e156899-d3d6-4cdf-a8aa-e944c7dcde4f","cell_type":"markdown","source":["## Consuming your model from another client\n","\n","It is awesome that fastAPI allows you to interact with your API through its built-in client. However, you might wonder how you can interact with your API using regular code and not some UI.\n","\n","There is a bonus section which shows how to code a minimal client in Python. This is useful to break down (in a very high level) what fastAPI's client is doing under the hood. However this section cannot be used within Coursera's environment. For this reason consider checking out the version of this lab that is meant to be run within your local computer. You can find it [here](https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/tree/main/course1/week1-ungraded-lab)."]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}